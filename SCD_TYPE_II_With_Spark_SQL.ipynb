{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8bddee-d69e-45d1-b85e-7248f8de4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5bc06b-8ef5-4054-b93d-df65433e944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/23 09:42:51 WARN Utils: Your hostname, DESKTOP-HG7VAEJ resolves to a loopback address: 127.0.1.1; using 172.28.202.26 instead (on interface eth0)\n",
      "25/07/23 09:42:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/23 09:42:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/23 09:42:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SCD_Type_II_SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961a9ef-99d3-4435-b844-0c61ceb2c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.option(\"header\", True).csv(\"dataset/customer.csv\")\n",
    "new_customer_df = spark.read.option(\"header\", True).csv(\"dataset/newcustomer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9e2360-aa70-4ef2-af3c-393335273c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "new_customer_df.createOrReplaceTempView(\"new_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc93b3f-e5f9-4445-ad9d-ea75572997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_date = \"2025-07-20\"\n",
    "previous_date = \"2025-07-19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a872958-fcdf-4e71-bf76-a3ee8566adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Expire old records where address changed\n",
    "expired_records = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    c.Customer_ID,\n",
    "    c.Name,\n",
    "    c.Address,\n",
    "    c.Start_Date,\n",
    "    '{previous_date}' AS End_Date,\n",
    "    'N' AS Is_Current\n",
    "FROM customer c\n",
    "JOIN new_customer n\n",
    "  ON c.Customer_ID = n.Customer_ID\n",
    "WHERE c.Address <> n.Address\n",
    "  AND c.Is_Current = 'Y'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c78f65-692b-4577-b24c-3fe01a07951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------+----------+----------+\n",
      "|Customer_ID|      Name| Address|Start_Date|  End_Date|Is_Current|\n",
      "+-----------+----------+--------+----------+----------+----------+\n",
      "|        101|  John Doe|New York|01-01-2022|2025-07-19|         N|\n",
      "|        102|Jane Smith| Chicago|15-03-2023|2025-07-19|         N|\n",
      "+-----------+----------+--------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expired_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20285a8a-c887-4e3b-ae9b-99b6e9c2c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Insert new updated records\n",
    "new_records = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    n.Customer_ID,\n",
    "    n.Name,\n",
    "    n.Address,\n",
    "    '{effective_date}' AS Start_Date,\n",
    "    '' AS End_Date,\n",
    "    'Y' AS Is_Current\n",
    "FROM customer c\n",
    "JOIN new_customer n\n",
    "  ON c.Customer_ID = n.Customer_ID\n",
    "WHERE c.Address <> n.Address\n",
    "  AND c.Is_Current = 'Y'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b155a3-9b75-4be1-905e-91112b1848c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+--------+----------+\n",
      "|Customer_ID|      Name|    Address|Start_Date|End_Date|Is_Current|\n",
      "+-----------+----------+-----------+----------+--------+----------+\n",
      "|        101|  John Doe|Los Angeles|2025-07-20|        |         Y|\n",
      "|        102|Jane Smith|     Dallas|2025-07-20|        |         Y|\n",
      "+-----------+----------+-----------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f984849-dea9-419b-b05c-1df3a8a6c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Keep unchanged records\n",
    "unchanged_records = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer\n",
    "WHERE Customer_ID NOT IN (\n",
    "    SELECT c.Customer_ID\n",
    "    FROM customer c\n",
    "    JOIN new_customer n\n",
    "      ON c.Customer_ID = n.Customer_ID\n",
    "    WHERE c.Address <> n.Address\n",
    "      AND c.Is_Current = 'Y'\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ddb456-4a42-4c53-85b7-da9c89c3981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+----------+--------+----------+\n",
      "|Customer_ID|        Name|      Address|Start_Date|End_Date|Is_Current|\n",
      "+-----------+------------+-------------+----------+--------+----------+\n",
      "|        103| Michael Lee|San Francisco|20-07-2021|    NULL|         Y|\n",
      "|        104| Emily Davis|       Boston|10-01-2024|    NULL|         Y|\n",
      "|        105|David Wilson|      Seattle|05-11-2023|    NULL|         Y|\n",
      "+-----------+------------+-------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unchanged_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763d87f4-f3b8-4712-984d-e5efebd95f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Combine final result\n",
    "final_df = expired_records.union(new_records).union(unchanged_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037aad34-0501-4955-af4b-6d9fcf4ebd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+----------+----------+----------+\n",
      "|Customer_ID|Name        |Address      |Start_Date|End_Date  |Is_Current|\n",
      "+-----------+------------+-------------+----------+----------+----------+\n",
      "|101        |John Doe    |New York     |01-01-2022|2025-07-19|N         |\n",
      "|101        |John Doe    |Los Angeles  |2025-07-20|          |Y         |\n",
      "|102        |Jane Smith  |Chicago      |15-03-2023|2025-07-19|N         |\n",
      "|102        |Jane Smith  |Dallas       |2025-07-20|          |Y         |\n",
      "|103        |Michael Lee |San Francisco|20-07-2021|NULL      |Y         |\n",
      "|104        |Emily Davis |Boston       |10-01-2024|NULL      |Y         |\n",
      "|105        |David Wilson|Seattle      |05-11-2023|NULL      |Y         |\n",
      "+-----------+------------+-------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.orderBy(\"Customer_ID\", \"Start_Date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b838cbe1-58e0-4a07-9c2e-8a73b3424f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.orderBy(\"Customer_ID\", \"Start_Date\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"scd_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
